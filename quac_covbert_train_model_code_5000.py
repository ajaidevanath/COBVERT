# -*- coding: utf-8 -*-
"""Quac_Covbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eneDWhM7P0E97zd4IQzz16GFGasnVRQN
"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import random
import os
import json

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

device

!nvidia-smi

!pip install transformers

!git clone https://github.com/amazon-research/qa-dataset-converter

!wget https://s3.amazonaws.com/my89public/quac/train_v0.2.json

!wget https://s3.amazonaws.com/my89public/quac/val_v0.2.json

!python qa-dataset-converter/quac/quac_to_squad.py --quac_file train_v0.2.json --output_file quac_train.json

!python qa-dataset-converter/quac/quac_to_squad.py --quac_file val_v0.2.json --output_file quac_dev.json

import math
jsonTrain = "./quac_train.json"
jsonVal = "./quac_dev.json"
 
file_data = pd.read_json(jsonTrain)
json_data = file_data['data']

size = math.floor(json_data.shape[0]*0.7)
trainContents = file_data['data'].loc[0:5000]

file_data = pd.read_json(jsonVal)
json_data = file_data['data']

size = math.floor(json_data.shape[0])
valContents = file_data['data'].loc[0:5000]

def jsonToLists(contents):
  texts=[]
  questions=[]
  answers=[]
  for data in contents:
    for txt in data['paragraphs']: # every text,
      text = txt['context']
      for qa in txt['qas']: # has many questions,
        question = qa['question']
        for answer in qa['answers']: # questions have answers.
          texts.append(text)
          questions.append(question)
          answers.append(answer)            
                  
  return texts,questions,answers

trainTexts,trainQuestions,trainAnswers=jsonToLists(trainContents)
#valTexts,valQuestions,valAnswers=jsonToLists(valContents)

def add_end_idx(answers, contexts):
    # get the character position at which every answer ends and store it
    for answer, context in zip(answers, contexts):
      end_idx = answer['answer_start'] + len(answer['text'])

      if context[answer['answer_start']:end_idx] == answer['text']:
        answer['answer_end'] = end_idx
      # Sometimes SQuAD answers are off by one or two characters, so ...
      elif context[answer['answer_start']-1:end_idx-1] == answer['text']:
        answer['answer_start'] = answer['answer_start'] - 1
        answer['answer_end'] = end_idx - 1
      elif context[answer['answer_start']-2:end_idx-2] == answer['text']:
        answer['answer_start'] = answer['answer_start'] - 2
        answer['answer_end'] = end_idx - 2
                
add_end_idx(trainAnswers, trainTexts)
#add_end_idx(valAnswers, valTexts)

from transformers import BertTokenizerFast,DistilBertTokenizerFast,AutoTokenizer

# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
#tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")

trainEncodings=tokenizer(trainTexts,trainQuestions, truncation=True, padding=True, max_length=512)
#valEncodings=tokenizer(valTexts,valQuestions, truncation=True, padding=True, max_length=512)

def add_token_positions(encodings, answers):
    # convert character start/end positions to token start/end positions.
    start_positions = []
    end_positions = []
    for i in range(len(answers)):
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        if(answers[i]['answer_end']==0):
            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'])) 
        else:
            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))

         # if None, the answer passage has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length
                   
        if end_positions[-1] is None:
            end_positions[-1] = tokenizer.model_max_length

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})

# apply function to our data
add_token_positions(trainEncodings, trainAnswers)
#add_token_positions(valEncodings, valAnswers)

class SquadDataset(torch.utils.data.Dataset):
  def __init__(self, encodings):
    self.encodings = encodings

  def __getitem__(self, idx):
    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

  def __len__(self):
    return len(self.encodings.input_ids)

# create the corresponding datasets
trainSet = SquadDataset(trainEncodings)
#valSet = SquadDataset(valEncodings)

from transformers import BertForQuestionAnswering,BertForQuestionAnswering
#model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
#model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')
m = BertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/project_ir/covbert_squad_10000-20000')
m.to(device)

from torch.utils.data import TensorDataset, DataLoader

batch_size = 30
trainDataloader = DataLoader(trainSet, batch_size=batch_size, shuffle=True)
#valDataloader = DataLoader(valSet, batch_size=batch_size, shuffle=False)

lr = 2e-5
optimizer = torch.optim.ASGD(model.parameters(), lr=lr)

from tqdm.auto import tqdm

epochs = 1
trainLosses=[]
torch.cuda.empty_cache()
for i in range(epochs):
    loop = tqdm(trainDataloader)
    for batch in loop:
        batchLosses = []
        m.train() # training mode

        for batch in trainDataloader:
    
            optimizer.zero_grad() # Delete previously stored gradients

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)

    
            outputs = m(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)

            batchLosses.append(outputs[0].item())  

            outputs[0].backward() # Perform backpropagation starting from the loss calculated in this epoch

            #nn.utils.clip_grad_norm_(m.parameters(),5.0) # Clip gradients to avoid vanishing gradients

            optimizer.step() # Update model's weights based on the gradients calculated during backprop

m.save_pretrained('/content/drive/MyDrive/quack_bert')

#Covebert model pretrained with squad and on cord 19

epochs = 1
trainLosses=[]
torch.cuda.empty_cache()
for i in range(epochs):
    loop = tqdm(trainDataloader)
    for batch in loop:
        batchLosses = []
        m.train() # training mode

        for batch in trainDataloader:
    
            optimizer.zero_grad() # Delete previously stored gradients

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)

    
            outputs = m(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)

            batchLosses.append(outputs[0].item())  

            outputs[0].backward() # Perform backpropagation starting from the loss calculated in this epoch

            nn.utils.clip_grad_norm_(model.parameters(),5.0) # Clip gradients to avoid vanishing gradients

            optimizer.step() # Update model's weights based on the gradients calculated during backprop

m.save_pretrained('/content/drive/MyDrive/quac_covbert_5000')

"""##Now we will take the next step of 5000 datapoints from the quac dataset and train this model above on those"""

import math
jsonTrain = "./quac_train.json"
jsonVal = "./quac_dev.json"
 
file_data = pd.read_json(jsonTrain)
json_data = file_data['data']

size = math.floor(json_data.shape[0]*0.7)
trainContents = file_data['data'].loc[5000:10000]

file_data = pd.read_json(jsonVal)
json_data = file_data['data']

size = math.floor(json_data.shape[0])
valContents = file_data['data'].loc[5000:10000]

def jsonToLists(contents):
  texts=[]
  questions=[]
  answers=[]
  for data in contents:
    for txt in data['paragraphs']: # every text,
      text = txt['context']
      for qa in txt['qas']: # has many questions,
        question = qa['question']
        for answer in qa['answers']: # questions have answers.
          texts.append(text)
          questions.append(question)
          answers.append(answer)            
                  
  return texts,questions,answers

trainTexts,trainQuestions,trainAnswers=jsonToLists(trainContents)

def add_end_idx(answers, contexts):
    # get the character position at which every answer ends and store it
    for answer, context in zip(answers, contexts):
      end_idx = answer['answer_start'] + len(answer['text'])

      if context[answer['answer_start']:end_idx] == answer['text']:
        answer['answer_end'] = end_idx
      # Sometimes SQuAD answers are off by one or two characters, so ...
      elif context[answer['answer_start']-1:end_idx-1] == answer['text']:
        answer['answer_start'] = answer['answer_start'] - 1
        answer['answer_end'] = end_idx - 1
      elif context[answer['answer_start']-2:end_idx-2] == answer['text']:
        answer['answer_start'] = answer['answer_start'] - 2
        answer['answer_end'] = end_idx - 2
                
add_end_idx(trainAnswers, trainTexts)
#add_end_idx(valAnswers, valTexts)

trainEncodings=tokenizer(trainTexts,trainQuestions, truncation=True, padding=True, max_length=512)

def add_token_positions(encodings, answers):
    # convert character start/end positions to token start/end positions.
    start_positions = []
    end_positions = []
    for i in range(len(answers)):
        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))
        if(answers[i]['answer_end']==0):
            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'])) 
        else:
            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))

         # if None, the answer passage has been truncated
        if start_positions[-1] is None:
            start_positions[-1] = tokenizer.model_max_length
                   
        if end_positions[-1] is None:
            end_positions[-1] = tokenizer.model_max_length

    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})

# apply function to our data
add_token_positions(trainEncodings, trainAnswers)
#add_token_positions(valEncodings, valAnswers)

class SquadDataset(torch.utils.data.Dataset):
  def __init__(self, encodings):
    self.encodings = encodings

  def __getitem__(self, idx):
    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

  def __len__(self):
    return len(self.encodings.input_ids)

# create the corresponding datasets
trainSet = SquadDataset(trainEncodings)
#valSet = SquadDataset(valEncodings)

from transformers import BertForQuestionAnswering,BertForQuestionAnswering
#model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
#model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')
m1 = BertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/quac_covbert_5000')
m1.to(device)

from torch.utils.data import TensorDataset, DataLoader

batch_size = 30
trainDataloader = DataLoader(trainSet, batch_size=batch_size, shuffle=True)
#valDataloader = DataLoader(valSet, batch_size=batch_size, shuffle=False)

lr = 2e-5
optimizer = torch.optim.ASGD(model.parameters(), lr=lr)

epochs = 1
trainLosses=[]
torch.cuda.empty_cache()
for i in range(epochs):
    loop = tqdm(trainDataloader)
    for batch in loop:
        batchLosses = []
        m1.train() # training mode

        for batch in trainDataloader:
    
            optimizer.zero_grad() # Delete previously stored gradients

            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)

    
            outputs = m1(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)

            batchLosses.append(outputs[0].item())  

            outputs[0].backward() # Perform backpropagation starting from the loss calculated in this epoch

            nn.utils.clip_grad_norm_(model.parameters(),5.0) # Clip gradients to avoid vanishing gradients

            optimizer.step() # Update model's weights based on the gradients calculated during backprop









models_test= BertForQuestionAnswering.from_pretrained('/content/drive/MyDrive/quac_covbert_5000')

def predictTheAnswer(text,question,model):
  inputs = tokenizer1.encode_plus(question, text, return_tensors='pt',max_length=512, truncation=True).to(device)

  outputs = model1(**inputs)
  answer_start = torch.argmax(outputs[0])  
  answer_end = torch.argmax(outputs[1]) + 1 

  answer = tokenizer1.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))
  return answer

def prediction(contents,model):
  predAnswers={}
  for data in contents['data']:
    for txt in data['paragraphs']: 
      text = txt['context']
      for qa in txt['qas']:
        qid = qa['id']
        question = qa['question']
        predAnswers[qid]=predictTheAnswer(text,question,model)
  return predAnswers

text='Daffy first appeared in Porkys Duck Hunt, released on April 17, 1937.[4] The cartoon was directed by Tex Avery and animated by Bob Clampett. Porkys Duck Hunt is a standard hunter/prey pairing, but Daffy (barely more than an unnamed bit player in this short) was something new to moviegoers: an assertive, completely unrestrained, combative protagonist. Clampett later recalled.At that time, audiences werent accustomed to seeing a cartoon character do these things. And so, when it hit the theaters it was an explosion. People would leave the theaters talking about this daffy duck.This early Daffy is less anthropomorphic and resembles a normal black duck. In fact, the only aspects of the character that have remained consistent through the years are his voice characterization by Mel Blanc; and his black feathers with a white neck ring. Blancs characterization of Daffy once held the world record for the longest characterization of one animated character by their original actor: 52 years.The origin of Daffys voice, with its lateral lisp, is a matter of some debate. One often-repeated official story is that it was modeled after producer Leon Schlesingers tendency to lisp. However, in Mel Blancs autobiography, Thats Not All Folks!, he contradicts that conventional belief, writing, It seemed to me that such an extended mandible would hinder his speech, particularly on words containing an s sound. Thus despicable became desth-picable.Daffys slobbery, exaggerated lisp was developed over time, and it is barely noticeable in the early cartoons. In Daffy Duck & Egghead, Daffy does not lisp at all except in the separately drawn set-piece of Daffy singing The Merry-Go-Round Broke Down in which just a slight lisp can be heard.In The Scarlet Pumpernickel (1950), Daffy has a middle name, Dumas as the writer of a swashbuckling script, a nod to Alexandre Dumas. Also, in the Baby Looney Tunes episode "The Tattletale", Granny addresses Daffy as Daffy Horatio Tiberius Duck. In The Looney Tunes Show (2011), the joke middle names Armando and Sheldon are used.'

predictTheAnswer(text,'granny addresses daffy?',model1.to(device))

from transformers import AutoTokenizer, AutoModelForQuestionAnswering

tokenizer1 = AutoTokenizer.from_pretrained("ixa-ehu/SciBERT-SQuAD-QuAC")

model1 = AutoModelForQuestionAnswering.from_pretrained("ixa-ehu/SciBERT-SQuAD-QuAC")

token